# utils/llm_client.py
import os
import logging
from typing import Dict, Any, Literal
from openai import OpenAI, OpenAIError
from anthropic import Anthropic, AnthropicError
from anthropic.types import TextBlock
from dotenv import load_dotenv

from utils.logger import get_logger

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

logger = get_logger("LLMClient")

class LLMClient:
    """OpenAI ê¸°ë°˜ LLM í˜¸ì¶œ ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self) -> None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        try:
            self.client = OpenAI(api_key=api_key)
        except Exception as e:
            logger.exception("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise e

    def chat(self, prompt: str, max_tokens: int = 3000) -> str:
        """GPT ì±— ì™„ë£Œ í˜¸ì¶œ"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens
            )
            content = response.choices[0].message.content
            return content if content else ""

        except OpenAIError as e:
            logger.error("OpenAI API ì˜¤ë¥˜ ë°œìƒ")
            raise e

        except Exception as e:
            logger.exception("LLM í˜¸ì¶œ ì‹¤íŒ¨")
            raise e


class ClaudeClient:
    """Claude (Anthropic) ê¸°ë°˜ LLM í˜¸ì¶œ ë˜í¼ í´ë˜ìŠ¤"""

    def __init__(self) -> None:
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        try:
            self.client = Anthropic(api_key=api_key)
        except Exception as e:
            logger.exception("Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise e

    def chat(self, prompt: str, max_tokens: int = 3000) -> str:
        """Claude ì±— ì™„ë£Œ í˜¸ì¶œ"""

        try:
            response = self.client.messages.create(
                model="claude-3-haiku-20240307",  # Claude 3 Haiku (ì €ë ´í•˜ê³  ë¹ ë¦„)
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # Claude API ì‘ë‹µ í˜•ì‹: response.content[0].text
            if response.content and len(response.content) > 0:
                content_block = response.content[0]
                # isinstanceë¡œ TextBlock í™•ì¸
                if isinstance(content_block, TextBlock):
                    return content_block.text
            return ""

        except AnthropicError as e:
            logger.error(f"Anthropic API ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e

        except Exception as e:
            logger.exception("Claude í˜¸ì¶œ ì‹¤íŒ¨")
            raise e


class HybridLLMClient:
    """
    í•˜ì´ë¸Œë¦¬ë“œ LLM í´ë¼ì´ì–¸íŠ¸
    - ì‘ì—… ìœ í˜•ì— ë”°ë¼ GPT ë˜ëŠ” Claudeë¥¼ ìë™ ì„ íƒ
    - ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜• ìœ ì§€
    """

    def __init__(self) -> None:
        # GPT ì´ˆê¸°í™” (í•„ìˆ˜)
        try:
            self.gpt_client = LLMClient()
        except Exception as e:
            logger.error("GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise e

        # Claude ì´ˆê¸°í™” (ì„ íƒ)
        try:
            self.claude_client = ClaudeClient()
            self.claude_available = True
            logger.info("âœ… Claude API ì‚¬ìš© ê°€ëŠ¥")
        except Exception as e:
            logger.warning(f"âš ï¸ Claude API ì‚¬ìš© ë¶ˆê°€ (GPTë§Œ ì‚¬ìš©): {e}")
            self.claude_available = False

    def chat(
        self, 
        prompt: str, 
        max_tokens: int = 3000,
        prefer_model: Literal["gpt", "claude", "auto"] = "auto",
        task_type: Literal["simple", "creative", "analytical"] = "simple"
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ ì„ íƒ
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            prefer_model: ì„ í˜¸ ëª¨ë¸ ("gpt", "claude", "auto")
            task_type: ì‘ì—… ìœ í˜•
                - simple: ë‹¨ìˆœ ì‘ì—… (GPT ì‚¬ìš©)
                - creative: ì°½ì˜ì  ì‘ì—… (Claude ìš°ì„ )
                - analytical: ë¶„ì„ ì‘ì—… (Claude ìš°ì„ )
        
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        
        # ëª…ì‹œì ìœ¼ë¡œ GPT ìš”ì²­
        if prefer_model == "gpt":
            logger.info("ğŸ¤– GPT-4o-mini ì‚¬ìš©")
            return self.gpt_client.chat(prompt, max_tokens)
        
        # ëª…ì‹œì ìœ¼ë¡œ Claude ìš”ì²­
        if prefer_model == "claude":
            if self.claude_available:
                logger.info("ğŸ§  Claude 3.5 Sonnet ì‚¬ìš©")
                return self.claude_client.chat(prompt, max_tokens)
            else:
                logger.warning("âš ï¸ Claude ë¶ˆê°€, GPTë¡œ ëŒ€ì²´")
                return self.gpt_client.chat(prompt, max_tokens)
        
        # auto: ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìë™ ì„ íƒ
        if task_type == "simple":
            logger.info("ğŸ¤– GPT-4o-mini ì‚¬ìš© (ë‹¨ìˆœ ì‘ì—…)")
            return self.gpt_client.chat(prompt, max_tokens)
        
        # creative, analytical ì‘ì—…ì€ Claude ìš°ì„ 
        if self.claude_available:
            logger.info(f"ğŸ§  Claude 3.5 Sonnet ì‚¬ìš© ({task_type} ì‘ì—…)")
            return self.claude_client.chat(prompt, max_tokens)
        else:
            logger.warning(f"âš ï¸ Claude ë¶ˆê°€, GPTë¡œ ëŒ€ì²´ ({task_type} ì‘ì—…)")
            return self.gpt_client.chat(prompt, max_tokens)

